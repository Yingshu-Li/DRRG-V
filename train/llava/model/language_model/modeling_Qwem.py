#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Callable, Optional, Union

import torch
from torch import nn
import torch.nn.functional as F
import numpy as np
from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.modeling_attn_mask_utils import AttentionMaskConverter
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    QuestionAnsweringModelOutput,
    SequenceClassifierOutputWithPast,
    TokenClassifierOutput,
)
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
try:
    from transformers.utils import LossKwargs  # some versions
except Exception:
    try:
        from transformers.loss.loss_utils import LossKwargs  # some other versions
    except Exception:
        class LossKwargs:  # fallback typing stub
            pass
from transformers.utils import auto_docstring, can_return_tuple, logging
from .configuration_qwen import Qwen3Config
from llava.cache import dLLMCache, dLLMCacheConfig

logger = logging.get_logger(__name__)


@use_kernel_forward_from_hub("RMSNorm")
class Qwen3RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Qwen3RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen3MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = False

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == "sliding_attention" else None

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Qwen3DecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)

        self.mlp = Qwen3MLP(config)
        self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)

        return outputs


@auto_docstring
class Qwen3PreTrainedModel(PreTrainedModel):
    config_class = Qwen3Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen3DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn_3 = True
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, Qwen3RMSNorm):
            module.weight.data.fill_(1.0)


class Qwen3RotaryEmbedding(nn.Module):
    def __init__(self, config: Qwen3Config, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


@auto_docstring
class Qwen3Model(Qwen3PreTrainedModel):
    def __init__(self, config: Qwen3Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3RotaryEmbedding(config=config)
        self.gradient_checkpointing = False
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> BaseModelOutputWithPast:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache
        if not isinstance(past_key_values, (type(None), Cache)):
            raise ValueError("The `past_key_values` should be either a `Cache` object or `None`.")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache()

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        # if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            # mask_kwargs = {
            #     "config": self.config,
            #     "input_embeds": inputs_embeds,
            #     "attention_mask": attention_mask,
            #     "cache_position": cache_position,
            #     "past_key_values": past_key_values,
            #     "position_ids": position_ids,
            # }
            # # Create the masks
            # causal_mask_mapping = {
            #     "full_attention": create_causal_mask(**mask_kwargs),
            # }
            # # The sliding window alternating layers are not always activated depending on the config
            # if self.has_sliding_layers:
            #     causal_mask_mapping["sliding_attention"] = create_sliding_window_causal_mask(**mask_kwargs)
        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position, is_causal=False) # Modify: MDM

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=causal_mask,
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **flash_attn_kwargs,
            )

            hidden_states = layer_outputs[0]

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )
    
    def _update_causal_mask(self, attention_mask, input_tensor, cache_position, is_causal=True):
        if self.config._attn_implementation == "flash_attention_2":
            if attention_mask is not None and 0.0 in attention_mask:
                return attention_mask
            return None

        dtype, device = input_tensor.dtype, input_tensor.device
        min_dtype = torch.finfo(dtype).min
        sequence_length = input_tensor.shape[1]
        if hasattr(self.layers[0].self_attn, "past_key_value"):  # static cache
            target_length = self.config.max_position_embeddings
        else:  # dynamic cache
            target_length = (
                attention_mask.shape[-1] if isinstance(attention_mask, torch.Tensor) else cache_position[-1] + 1
            )

        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)
        if sequence_length != 1:
            causal_mask = torch.triu(causal_mask, diagonal=1)
        
        if is_causal == False:
            causal_mask = torch.zeros((sequence_length, target_length), dtype=dtype, device=device)
        
        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)
        causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)
        if attention_mask is not None:
            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
            if attention_mask.dim() == 2:
                # The position with 1 in attention_mask represents the place to be attended to, so here we need to mask the place where attention_mask is 0
                mask_length = attention_mask.shape[-1]
                padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)
                causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)
            elif attention_mask.dim() == 4:
                # The position with 1 in attention_mask represents the place to be attended to, so here we need to mask the place where attention_mask is 0
                # backwards compatibility: we allow passing a 4D attention mask shorter than the input length with
                # cache. In that case, the 4D attention mask attends to the newest tokens only.
                if attention_mask.shape[-2] < cache_position[0] + sequence_length:
                    offset = cache_position[0]
                else:
                    offset = 0
                mask_shape = attention_mask.shape
                mask_slice = (attention_mask.eq(0.0)).to(dtype=dtype) * min_dtype
                causal_mask[
                    : mask_shape[0], : mask_shape[1], offset : mask_shape[2] + offset, : mask_shape[3]
                ] = mask_slice

        if (
            self.config._attn_implementation == "sdpa"
            and attention_mask is not None
            and attention_mask.device.type == "cuda"
        ):
            # TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).
            is_tracing = (
                torch.jit.is_tracing()
                or isinstance(input_tensor, torch.fx.Proxy)
                or (hasattr(torch, "_dynamo") and torch._dynamo.is_compiling())
            )
            if not is_tracing and torch.any(attention_mask != 1):
                # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when
                # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.
                # Details: https://github.com/pytorch/pytorch/issues/110213
                causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)

        return causal_mask

# class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...


class LengthDiscriminatorNetwork(nn.Module):
    """
    Length discriminator network with 4 transformer blocks followed by a length prediction head.
    """
    def __init__(self, config: Qwen3Config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        
        # Create a lightweight config for the discriminator transformer blocks
        # Use 4 layers as specified
        self.num_layers = 4
        
        # Create 4 transformer decoder layers
        self.layers = nn.ModuleList([
            Qwen3DecoderLayer(config, layer_idx=i) for i in range(self.num_layers)
        ])
        
        # Layer normalization before the head
        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # Length prediction head (same as original)
        self.length_head = nn.Linear(config.hidden_size, 1, bias=False)
        
        # Rotary embedding for positional encoding
        self.rotary_emb = Qwen3RotaryEmbedding(config=config)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        Args:
            hidden_states: Input hidden states from the main model [batch_size, seq_len, hidden_size]
            attention_mask: Attention mask [batch_size, seq_len]
            position_ids: Position IDs [batch_size, seq_len]
            cache_position: Cache position for generation
        
        Returns:
            eos_logits: Length prediction logits [batch_size, seq_len]
        """
        # Get positional embeddings
        if position_ids is None and cache_position is not None:
            position_ids = cache_position.unsqueeze(0)
        elif position_ids is None:
            batch_size, seq_length = hidden_states.shape[:2]
            position_ids = torch.arange(
                seq_length, dtype=torch.long, device=hidden_states.device
            ).unsqueeze(0).expand(batch_size, -1)
        
        # Get rotary embeddings
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        if attention_mask is not None:
            if cache_position is None:
                batch_size, seq_length = hidden_states.shape[:2]
                cache_position = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device)
            
            dtype, device = hidden_states.dtype, hidden_states.device
            min_dtype = torch.finfo(dtype).min
            sequence_length = hidden_states.shape[1]
            target_length = attention_mask.shape[-1] if attention_mask.dim() >= 2 else cache_position[-1] + 1
            
            # Create causal mask (all zeros for full attention in discriminator)
            causal_mask = torch.zeros((sequence_length, target_length), dtype=dtype, device=device)
            causal_mask = causal_mask[None, None, :, :].expand(hidden_states.shape[0], 1, -1, -1)
            
            # Apply padding mask if attention_mask is provided
            if attention_mask.dim() == 2:
                mask_length = attention_mask.shape[-1]
                # Convert to proper dtype first
                attn_mask_expanded = attention_mask[:, None, None, :].to(dtype=dtype)
                padding_mask = attn_mask_expanded.eq(0.0)
                causal_mask = causal_mask.masked_fill(padding_mask, min_dtype)
            
            attention_mask = causal_mask
        # Pass through transformer layers
        for layer in self.layers:
            layer_outputs = layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=None,  # No caching for discriminator
                output_attentions=False,
                use_cache=False,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs
            )
            hidden_states = layer_outputs[0]
        
        # Apply final layer norm
        hidden_states = self.norm(hidden_states)
        
        # Apply length prediction head
        eos_logits = self.length_head(hidden_states).squeeze(-1)  # [batch_size, seq_len]
        
        return eos_logits


@auto_docstring
class Qwen3ForCausalLM(Qwen3PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen3Model(config)
        self.vocab_size = config.vocab_size
        self.stage = getattr(config, 'stage', 3)
        self.length_pred_len = getattr(config, 'length_pred_len', 300)
        # Use the new length discriminator network instead of a simple linear layer
        self.length_discriminator = LengthDiscriminatorNetwork(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model
    
    def _build_conversation_mask_optimized(self, conversation_ids):
        # Reshape conversation_ids for broadcasting
        ids_i = conversation_ids.unsqueeze(-1)  # [batch_size, seq_len, 1]
        ids_j = conversation_ids.unsqueeze(-2)  # [batch_size, 1, seq_len]

        # Use broadcasting to compare all pairs of conversation IDs
        conv_mask = (ids_j <= ids_i)  # [batch_size, seq_len, seq_len]

        # Add the attention head dimension
        return conv_mask.unsqueeze(1)  # [batch_size, 1, seq_len, seq_len]
    
    @staticmethod
    def add_gumbel_noise(logits, temperature):
        '''
        The Gumbel max is a method for sampling categorical distributions.
        According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.
        Thus, we use float64.
        '''
        if temperature == 0:
            # When temperature=0, we can directly return the original logits. 
            # without any noise or transformation
            return logits
        
        # use float64 for more stable computation
        logits = logits.to(torch.float64)
        noise = torch.rand_like(logits, dtype=torch.float64)
        gumbel_noise = (- torch.log(noise)) ** temperature
        return logits.exp() / gumbel_noise

    @staticmethod
    def get_num_transfer_tokens(mask_index, steps):
        '''
        Precompute the number of tokens to transition at each step.
        Optimized to be more efficient.
        '''
        mask_num = mask_index.sum(dim=1, keepdim=True)
        base = mask_num // steps
        remainder = mask_num % steps

        # Create tensor once and modify in-place (via clone)
        num_transfer_tokens = base.expand(-1, steps).clone()

        # Handle remainder more efficiently
        if remainder.sum() > 0: # Optimization: only proceed if there are remainders
            indices = torch.arange(steps, device=mask_index.device)
            # Create mask using broadcasting
            # indices shape: [steps] -> [1, steps]
            # remainder shape: [batch_size, 1]
            # mask shape: [batch_size, steps]
            mask = indices.unsqueeze(0) < remainder
            num_transfer_tokens[mask] += 1

        return num_transfer_tokens.to(torch.int64)

    @staticmethod
    def get_masked_indices_from_embeds(noisy_embeds, masked_embed):
        # Get shape information
        b, l, d = noisy_embeds.shape
        # Expand masked_embed to the same shape as noisy_embeds [b, l, d]
        masked_embed_expanded = masked_embed.expand(b, l, d)
        # Calculate absolute difference
        abs_diff = torch.abs(noisy_embeds - masked_embed_expanded)
        # Calculate tolerance boundary (atol + rtol * abs(masked_embed))
        tolerance = 1e-5 + 1e-5 * torch.abs(masked_embed_expanded)
        # Check if all dimensions at each position are within tolerance
        # all(dim=-1) ensures all dimensions of each embedding meet the condition
        masked_indices = (abs_diff <= tolerance).all(dim=-1)
        
        return masked_indices
    
    @ torch.no_grad()
    def generate(self, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,
                cfg_scale=0., remasking='low_confidence', mask_id=126336):
        '''
        Args:
            prompt: A tensor of shape (1, l).
            steps: Sampling steps, less than or equal to gen_length.
            gen_length: Generated answer length.
            block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
            temperature: Categorical distribution sampling temperature.
            cfg_scale: Unsupervised classifier-free guidance scale.
            remasking: Remasking strategy. 'low_confidence' or 'random'.
            mask_id: The toke id of [MASK] is 126336.
        '''
        x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(prompt.device)
        x[:, :prompt.shape[1]] = prompt.clone()

        prompt_index = (x != mask_id)

        assert gen_length % block_length == 0
        num_blocks = gen_length // block_length

        assert steps % num_blocks == 0
        steps = steps // num_blocks

        for num_block in range(num_blocks):
            block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)
            num_transfer_tokens = self.get_num_transfer_tokens(block_mask_index, steps)
            for i in range(steps):
                mask_index = (x == mask_id)
                if cfg_scale > 0.:
                    un_x = x.clone()
                    un_x[prompt_index] = mask_id
                    x_ = torch.cat([x, un_x], dim=0)
                    logits = self.model(x_).logits
                    logits, un_logits = torch.chunk(logits, 2, dim=0)
                    logits = un_logits + (cfg_scale + 1) * (logits - un_logits)
                else:
                    logits = self.model(x).logits

                logits_with_noise = self.add_gumbel_noise(logits, temperature=temperature)
                x0 = torch.argmax(logits_with_noise, dim=-1) # b, l

                if remasking == 'low_confidence':
                    p = F.softmax(logits.to(torch.float64), dim=-1)
                    x0_p = torch.squeeze(
                        torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l
                elif remasking == 'random':
                    x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
                else:
                    raise NotImplementedError(remasking)

                x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf

                x0 = torch.where(mask_index, x0, x)
                confidence = torch.where(mask_index, x0_p, -np.inf)

                transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
                for j in range(confidence.shape[0]):
                    _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])
                    transfer_index[j, select_index] = True
                x[transfer_index] = x0[transfer_index]

        return x

    @torch.no_grad()
    def generate_with_embeds(self, inputs_embeds, steps=128, gen_length=128, block_length=128, temperature=0.,
        cfg_scale=0., remasking='low_confidence', mask_id=126336, tokenizer=None, use_length_prediction=False, stopping_criteria=None, generation_suffix=None, image_token_mask=None, **kwargs):
        '''
        Args:
            inputs_embeds: A tensor of shape (1, l, d).
            steps: Sampling steps, less than or equal to gen_length.
            gen_length: Generated answer length.
            block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
            temperature: Categorical distribution sampling temperature.
            cfg_scale: Unsupervised classifier-free guidance scale.
            remasking: Remasking strategy. 'low_confidence' or 'random'.
            mask_id: The toke id of [MASK] is 126336.
            use_length_prediction: Whether to use length prediction head.
            generation_suffix: (str or None) Generation suffix, such as "The answer is xxx", will be appended to the end
        '''
        # Use mixed precision for faster computation
        with torch.cuda.amp.autocast(enabled=True):
            # Handle generation suffix
            suffix_embeds = None
            suffix_token_ids = None
            suffix_len = 0
            if generation_suffix is not None and tokenizer is not None and len(generation_suffix) > 0:
                # Encode as token id
                suffix_token_ids = tokenizer.encode(generation_suffix, add_special_tokens=False)
                suffix_token_ids = torch.tensor(suffix_token_ids, dtype=torch.long, device=inputs_embeds.device).unsqueeze(0)  # (1, s)
                # Convert to embedding
                suffix_embeds = self.model.embed_tokens(suffix_token_ids)  # (1, s, d)
                suffix_len = suffix_embeds.shape[1]
            else:
                suffix_len = 0
            if use_length_prediction and hasattr(self, 'length_discriminator'):
                # Prepare embeddings for length prediction
                masked_embed = self.model.embed_tokens(torch.tensor([mask_id], device=inputs_embeds.device))
                prompt_len = inputs_embeds.shape[1]
                
                # Create full sequence: prompt + gen_length masks
                length_pred_embeds = masked_embed.repeat(1, prompt_len + gen_length, 1)
                length_pred_embeds[:, :prompt_len] = inputs_embeds.clone()
                
                # Create attention mask for length prediction
                length_attn_mask = torch.ones((1, prompt_len + gen_length), device=inputs_embeds.device, dtype=torch.long)
                
                # Forward pass through the model
                outputs = self.model(
                    inputs_embeds=length_pred_embeds,
                    attention_mask=length_attn_mask,
                    use_cache=False,
                ) 
                hidden_states = outputs.last_hidden_state  # (1, prompt_len + gen_length, hidden_size)
                # Use the length discriminator network instead of simple linear head
                eos_scores = self.length_discriminator(hidden_states, attention_mask=length_attn_mask).float()  # (1, prompt_len + gen_length)
            # Only consider the mask region for EOS prediction
                mask_region_start = prompt_len
                mask_region_end = prompt_len + gen_length
                mask_scores = eos_scores[:, mask_region_start:mask_region_end]  # (1, gen_length)
            
                # Find the position with highest EOS score in the mask region
                predicted_eos_idx = torch.argmax(mask_scores, dim=-1).item()  # Relative to mask_region_start
                predicted_eos_pos = mask_region_start + predicted_eos_idx  # Absolute position
                
                # Update actual_gen_length (number of masks to keep)
                actual_gen_length = predicted_eos_idx + 1  # +1 because we include the EOS position
                
                print(f"[Length Prediction] Original gen_length: {gen_length}, Predicted EOS at position: {predicted_eos_pos}, Actual gen_length: {actual_gen_length}")
                gen_length = actual_gen_length       
            # Create input in embedding space
            total_length = inputs_embeds.shape[1] + gen_length + suffix_len
            masked_embed = self.model.embed_tokens(torch.tensor([mask_id]).to(inputs_embeds.device)) # shape (1, d)
            x_embeds = masked_embed.repeat(1, total_length, 1).to(inputs_embeds.device) # shape (1, l + gen_length + suffix_len, d)
            x_embeds[:, :inputs_embeds.shape[1]] = inputs_embeds.clone()
            if suffix_embeds is not None:
                x_embeds[:, -suffix_len:] = suffix_embeds
            # Create a tracking tensor for token IDs for final output
            x = torch.full((1, total_length), mask_id, dtype=torch.long, device=inputs_embeds.device)
            if suffix_token_ids is not None:
                x[:, -suffix_len:] = suffix_token_ids

            # prompt_index: A tensor of shape (1, l + gen_length + suffix_len) where the first l elements are 1 (representing the prompt) 
            # and the remaining gen_length+suffix_len elements are 0 (representing the generated part)
            prompt_index = torch.zeros((1, total_length), dtype=torch.bool, device=inputs_embeds.device)
            prompt_index[:, :inputs_embeds.shape[1]] = 1 # shape (1, l + gen_length + suffix_len)
            block_length = gen_length
            assert gen_length % block_length == 0
            num_blocks = gen_length // block_length

            assert steps % num_blocks == 0
            steps = steps // num_blocks

            # New: Initialize stop position variable (default to maximum length)
            stop_position = inputs_embeds.shape[1] + gen_length
            found_stop_seq = False
        
            stop_tokens = []
            if stopping_criteria is not None:
                assert tokenizer is not None, "tokenizer is required when stopping_criteria is not None"
                for stop_str in stopping_criteria:
                    # Use tokenizer to convert stop words to token IDs
                    tokens = tokenizer.encode(stop_str, add_special_tokens=False)
                    stop_tokens.append(tokens)

            feature_cache = dLLMCache()
            feature_cache.reset_cache(inputs_embeds.shape[1])
            for num_block in range(num_blocks):
                # Create mask index for the current block
                block_start = inputs_embeds.shape[1] + num_block * block_length
                block_end = inputs_embeds.shape[1] + (num_block + 1) * block_length

                # If a stop word is found and the stop word position is before the current block, do not process the current block
                if found_stop_seq and stop_position <= block_start:
                    break
                
                block_embeds = x_embeds[:, block_start:block_end]
                block_mask_index = torch.all(torch.abs(block_embeds - masked_embed) < 1e-5, dim=2)
                
                num_transfer_tokens = self.get_num_transfer_tokens(block_mask_index, steps)
                
                for i in range(steps):
                    # Determine which positions are mask embeddings
                    mask_index = torch.all(torch.abs(x_embeds - masked_embed) < 1e-5, dim=2)

                    # If a stop word has been found, check if the masks before the stop word are all filled
                    if found_stop_seq:
                        # Get the mask state before the stop word
                        pre_stop_masks = mask_index[0, inputs_embeds.shape[1]:stop_position]
                        # If the masks before the stop word are all filled, exit generation
                        if not pre_stop_masks.any():
                            break
                    
                    # Check if there are any masks left to fill in the current block
                    current_block_masks = mask_index[0, block_start:block_end]
                    if not current_block_masks.any():
                        break
                    
                    # Handle CFG
                    if cfg_scale > 0.:
                        un_embeds = x_embeds.clone() # shape (1, l + gen_length + suffix_len, d)
                        un_mask = prompt_index.unsqueeze(-1).expand_as(x_embeds)  # shape (1, l + gen_length + suffix_len, d)
                        un_embeds[un_mask] = masked_embed.repeat(x_embeds.shape[0],x_embeds.shape[1],1)[un_mask] # Use repeat to avoid the complexity of expand_as
                        combined_embeds = torch.cat([x_embeds, un_embeds], dim=0)
                        
                        # Forward pass
                        outputs = self.model(inputs_embeds=combined_embeds)
                        logits = self.lm_head(outputs[0]).float()                       
                        
                        # Split and apply CFG
                        logits, un_logits = torch.chunk(logits, 2, dim=0)
                        logits = un_logits + (cfg_scale + 1) * (logits - un_logits)
                    else:
                        # Forward pass (required every step as x_embeds changes)
                        outputs = self.model(inputs_embeds=x_embeds)
                        logits = self.lm_head(outputs[0]).float()
                    
                    for token_id in [126081, 126080, 126346, 126347]:
                        logits[:, :, token_id] = torch.where(mask_index, -float('inf'), logits[:, :, token_id])
                    
                    # Add noise and get the most likely token
                    logits_with_noise = self.add_gumbel_noise(logits, temperature=temperature) # shape (1, l + gen_length + suffix_len, vocab_size)
                    x0 = torch.argmax(logits_with_noise, dim=-1) # 1, l + gen_length + suffix_len
                    
                    # Get confidence scores
                    if remasking == 'low_confidence':
                        p = F.softmax(logits.to(torch.float64), dim=-1) # shape (1, l + gen_length + suffix_len, vocab_size)
                        x0_p = torch.squeeze(
                            torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # 1, l + gen_length + suffix_len represents the confidence of each x0
                    elif remasking == 'random':
                        x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
                    else:
                        raise NotImplementedError(remasking)
                    
                    # If a stop word is found, only process positions before the stop word
                    if found_stop_seq:
                        x0_p[:, stop_position:] = -np.inf
                    else:
                        # Prevent processing future blocks
                        x0_p[:, block_end:] = -np.inf

                    # Do not allow the generated suffix part to be overwritten
                    if suffix_len > 0:
                        x0_p[:, -suffix_len:] = -np.inf

                    # Update predictions only at mask positions
                    x0_embeds = self.model.embed_tokens(x0) # shape (1, l + gen_length + suffix_len, d)
                    x0_embeds = torch.where(mask_index.unsqueeze(-1).expand_as(x_embeds), x0_embeds, x_embeds)
                    x0 = torch.where(mask_index, x0, x) # shape (1, l + gen_length + suffix_len) 
                    
                    # Calculate confidence and determine transfer index
                    confidence = torch.where(mask_index, x0_p, -np.inf)
                    
                    transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
                    for j in range(confidence.shape[0]):
                        _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])
                        transfer_index[j, select_index] = True
                    
                    # Update embeddings and token IDs
                    x_embeds[transfer_index] = x0_embeds[transfer_index]
                    x[transfer_index] = x0[transfer_index]

                    # New: Check for stop words after each update
                    if stopping_criteria is not None:
                        # Only check the generated part (excluding the suffix)
                        generated_part = x[0, inputs_embeds.shape[1]:inputs_embeds.shape[1]+gen_length]
                        current_stop_position = None
                        
                        for stop_seq in stop_tokens:
                            if not isinstance(stop_seq, list):
                                stop_seq = [stop_seq]
                            # Check if the generated sequence contains stop words
                            for start_idx in range(generated_part.size(0) - len(stop_seq) + 1):
                                if torch.all(generated_part[start_idx:start_idx + len(stop_seq)] == torch.tensor(stop_seq, device=x.device)):
                                    # Calculate the position of the currently found stop word
                                    current_position = inputs_embeds.shape[1] + start_idx
                                    # If it is the first time a stop word is found, or this stop word is earlier than the previously found one
                                    if not found_stop_seq or current_position < stop_position:
                                        stop_position = current_position
                                        found_stop_seq = True
                                    break
                            if found_stop_seq and current_stop_position is None:
                                break

            # Return the generated result, up to stop_position, and append the suffix
            if found_stop_seq:
                if suffix_len > 0:
                    return torch.cat([
                        x[:, inputs_embeds.shape[1]:stop_position], 
                        x[:, -suffix_len:]
                    ], dim=1)
                else:
                    return x[:, inputs_embeds.shape[1]:stop_position]
            else:
                if suffix_len > 0:
                    return torch.cat([
                        x[:, inputs_embeds.shape[1]:inputs_embeds.shape[1]+gen_length], 
                        x[:, -suffix_len:]
                    ], dim=1)
                else:
                    return x[:, inputs_embeds.shape[1]:inputs_embeds.shape[1]+gen_length]
    
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        conversation_ids: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> CausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size].`

        conversation_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Optional tensor used to indicate conversation turns / message grouping inside each sequence. When
            provided, a conversation mask will be constructed so that tokens are only allowed to attend to tokens
            from the same or earlier conversation turn (i.e. attention is restricted by conversation turn id).
            If `attention_mask` is also provided, the two masks will be combined (element-wise) so both padding
            and conversation boundaries are respected. This is useful for multi-dialogue or multi-message inputs
            where you want to prevent cross-turn attention beyond permitted boundaries.

        Example:
        **kwargs: Unpack[KwargsForCausalLM],
        ```python
        >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

        >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
        >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )


        def forward_process_embeds(input_embeds, labels, eps=1e-3):
            b, l, d = input_embeds.shape
            t = torch.rand(b, device=input_embeds.device)
            p_mask = (1 - eps) * t + eps
            p_mask = p_mask[:, None].repeat(1, l)

            masked_indices = torch.rand((b, l), device=input_embeds.device) < p_mask
            # Add label condition filtering
            valid_mask = (labels != -100) # Create valid encoding
            masked_indices = masked_indices & valid_mask # Combine random encoding and valid encoding
            # Magic number 126336 stands for the tokenizer special token,
            # Magic embeddings, which is used for [MASK] token here,
            masked_embed = self.model.embed_tokens(torch.tensor([126336]).to(input_embeds.device))
            noisy_embeds = torch.where(masked_indices.unsqueeze(-1), masked_embed, input_embeds)

            return noisy_embeds, p_mask, masked_embed

        if self.stage in (1, 2):
            noisy_embeds, p_mask, masked_embed = forward_process_embeds(inputs_embeds, labels)
            bsz, seq_len, _ = inputs_embeds.shape
            masked_indices = self.get_masked_indices_from_embeds(noisy_embeds, masked_embed) # shape (b, l)
            prompt_index = (labels == -100).to(torch.int64)
            target_mask = (1 - prompt_index).bool() # shape (b, l)
            masked_indices_env = (~masked_indices) & (target_mask)
            noisy_data_length = torch.sum((1-prompt_index), dim=-1, keepdim=True) # shape (b, 1)
            noisy_data_length = noisy_data_length.repeat(1, noisy_embeds.shape[1]) # shape (b, l)
            noisy_embeds_inv = torch.where(masked_indices_env.view(bsz,seq_len,1),masked_embed,inputs_embeds)
            labels_env = labels.clone()
            labels_env[masked_indices]= -100
            labels[masked_indices_env]= -100
            labels =  torch.cat([labels,labels_env])
            noisy_embeds = torch.cat([noisy_embeds,noisy_embeds_inv])
            p_mask_inv = 1.0 - p_mask
            p_mask = torch.cat([p_mask, p_mask_inv], dim=0)
            noisy_data_length = noisy_data_length.repeat(2, 1)
            if conversation_ids is not None: 
                conversation_mask = self._build_conversation_mask_optimized(conversation_ids)
                if attention_mask is not None:
                    # 1. Dimension expansion
                    attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # (batch, length) -> (batch, 1, 1, length)
                    attention_mask = attention_mask.expand_as(conversation_mask) # (batch, 1, 1, length) -> (batch, 1, length, length)
                    # 2. Mask combination (element-wise multiplication)
                    combined_mask = conversation_mask * attention_mask
                else:
                    # If attention_mask is None, directly use conversation_mask
                    combined_mask = conversation_mask 
                attention_mask = combined_mask

            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=noisy_embeds,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                cache_position=cache_position,
                **kwargs,
            )

            hidden_states = outputs.last_hidden_state
            pretraining_tp = getattr(self.config, "pretraining_tp", 1)
            if pretraining_tp > 1:
                lm_head_slices = self.lm_head.weight.split(self.vocab_size // pretraining_tp, dim=0)
                logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(pretraining_tp)]
                logits = torch.cat(logits, dim=-1)
            else:
                logits = self.lm_head(hidden_states)
            logits = logits.float()
            logits = logits.transpose(1, 2)
            loss = None
            if labels is not None:
                token_loss = F.cross_entropy(logits, labels, ignore_index=-100,
                                            reduction='none') / p_mask
                loss = torch.sum(token_loss / noisy_data_length) / labels.shape[0]

            if not return_dict:
                output = (logits,) + outputs[1:]
                return (loss,) + output if loss is not None else output
            return CausalLMOutputWithPast(
                loss=loss,
                logits=logits,
                past_key_values=outputs.past_key_values,
                hidden_states=outputs.hidden_states,
                attentions=outputs.attentions,
            )
        
        if self.stage == 3:
            # if input_ids is None:
            #     raise ValueError("input_ids is required when stage == 3.")

            length_pred_len = self.length_pred_len
            eos_token_id = getattr(self.config, "eos_token_id", None)
            if eos_token_id is None:
                raise ValueError("config.eos_token_id must be set when stage == 3.")
            pad_token_id = getattr(self.config, "pad_token_id", None)
            mask_id = kwargs.pop("mask_id", 126336)
            # prompt_index = (labels == -100).to(torch.int64)
            # prompt_embeds = inputs_embeds[:, :prompt_index, :]
            prompt_lens = ((labels == -100) & (attention_mask == 1)).sum(dim=1)
            bsz, seq_len,  _ = inputs_embeds.shape
            masked_embed = self.model.embed_tokens(torch.tensor([mask_id], device=inputs_embeds.device))
            eos_embed = self.model.embed_tokens(torch.tensor([eos_token_id], device=inputs_embeds.device))
            eos_label_mask = (labels == eos_token_id)
            if eos_label_mask.sum() == 0:
                raise ValueError("No EOS token found in labels when stage == 3.")
            
            pos_indices = torch.arange(labels.shape[1], device=labels.device).unsqueeze(0).expand(bsz, -1)
            eos_label_positions = torch.where(eos_label_mask, pos_indices, torch.full_like(pos_indices, -1))
            eos_pos_orig = eos_label_positions.max(dim=1).values
            # eos_pos_orig = torch.zeros(bsz, dtype=torch.long, device=inputs_embeds.device)
            # for i in range(bsz):
            #     p_len = int(prompt_lens[i])
            #     answer_embeds = inputs_embeds[i, p_len:, :]
            #     abs_diff = torch.abs(answer_embeds - eos_embed)
            #     tolerance = 1e-5 + 1e-5 * torch.abs(eos_embed)
            #     is_eos = (abs_diff <= tolerance).all(dim=-1) 
            #     answer_len = answer_embeds.shape[0]
            #     pos_in_answer = torch.arange(answer_len, device=inputs_embeds.device)
            #     eos_pos_in_answer = torch.where(is_eos, pos_in_answer, torch.full_like(pos_in_answer, -1))
            #     max_eos_pos_in_answer = eos_pos_in_answer.max()
            #     eos_pos_orig[i] = p_len + max_eos_pos_in_answer
            mask_embeds = masked_embed.repeat(bsz, length_pred_len, 1)
            pad_embed_unit = self.model.embed_tokens(torch.tensor([pad_token_id], device=inputs_embeds.device))
            max_total_len = int(prompt_lens.max().item() + length_pred_len)
            length_embeds = pad_embed_unit.repeat(bsz, max_total_len, 1)
            attention_mask = torch.zeros((bsz, max_total_len), device=inputs_embeds.device, dtype=torch.long)
            
            for i in range(bsz):
                p_len = int(prompt_lens[i])
                valid_prompt = inputs_embeds[i, :p_len, :]
                mask_region = mask_embeds[i]
                combined = torch.cat([valid_prompt, mask_region], dim=0)
                actual_len_i = p_len + length_pred_len
                length_embeds[i, :actual_len_i, :] = combined
                attention_mask[i, :actual_len_i] = 1
            actual_len = prompt_lens + length_pred_len
            # eos_mask = input_ids == eos_token_id
            # if eos_mask.sum() == 0:
            #     raise ValueError("No eos_token_id found in input_ids when stage == 3.")
            # pos_orig = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(bsz, -1)
            # eos_pos = torch.where(eos_mask, pos_orig, torch.full_like(pos_orig, -1))
            # eos_pos_orig = eos_pos.max(dim=1).values
            if (eos_pos_orig < 0).any():
                print("No eos_token_id found in input_ids when stage == 3.")
            if (eos_pos_orig >= actual_len).any():
                print("eos position is outside (prompt_len + length_pred_len) for some samples.")
            position_ids_new = torch.arange(max_total_len, device=inputs_embeds.device).unsqueeze(0).expand(bsz, -1)
            mask_start = prompt_lens.unsqueeze(1)
            mask_end   = (prompt_lens + length_pred_len).unsqueeze(1)
            mask_length = (position_ids_new >= mask_start) & (position_ids_new < mask_end)
            prompt_mask = position_ids_new < prompt_lens.unsqueeze(1)         
            pad_mask    = attention_mask == 0                     
            ignore_mask = prompt_mask | pad_mask
            gt = torch.zeros((bsz, max_total_len), device=inputs_embeds.device, dtype=inputs_embeds.dtype)
            # gt[ignore_mask] = neg_inf
            batch_idx = torch.arange(bsz, device=inputs_embeds.device)
            gt[batch_idx, eos_pos_orig] = 1.0

            outputs = self.model(
                input_ids=None,
                attention_mask=attention_mask,
                position_ids=position_ids_new,
                past_key_values=past_key_values,
                inputs_embeds=length_embeds,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                cache_position=cache_position,
                **kwargs,
            )

            hidden_states = outputs.last_hidden_state
            # Use the length discriminator network
            eos_logits = self.length_discriminator(
                hidden_states, 
                attention_mask=attention_mask,
                position_ids=position_ids_new,
                cache_position=cache_position
            )
            eos_logits = eos_logits.float()
            neg = torch.finfo(eos_logits.dtype).min
            scores = eos_logits.masked_fill(~mask_length, neg)
            loss = None
            if gt is not None:
                target = gt.argmax(dim=1).long()
                print(f"Prompt Len: {prompt_lens.tolist()}, GT EOS: {eos_pos_orig.tolist()}, Pred EOS: {scores.argmax(dim=1).tolist()}")
                length_loss = F.cross_entropy(scores, target, reduction='mean')    
            # gt_delta = (eos_pos_orig - prompt_lens).float()
            # scores = []
            # for i in range(bsz):
            #     p = int(prompt_lens[i].item())
            #     scores.append(eos_logits[i, p:p + length_pred_len])
            # scores = torch.stack(scores, dim=0)     
            # valid = (eos_pos_orig >= 0) & (gt_delta >= 0) & (gt_delta < length_pred_len)
            # print(f"Prompt Len: {prompt_lens.tolist()}, GT EOS: {eos_pos_orig.tolist()}, Pred EOS: {scores.argmax(dim=1).tolist()}")
            # if valid.sum().item() == 0:
            #     length_loss = eos_logits.sum() * 0.0
            # else:
            #     sr = scores[valid]
            #     gd = gt_delta[valid]
            #     prob = torch.softmax(sr, dim=-1)
            #     pos = torch.arange(length_pred_len, device=sr.device, dtype=sr.dtype)
            #     pred_delta = (prob * pos[None, :]).sum(dim=-1)
            #     length_loss = torch.nn.functional.smooth_l1_loss(pred_delta, gd, reduction="mean")     
            return CausalLMOutputWithPast(
                loss=length_loss,
                logits=eos_logits,
                past_key_values=outputs.past_key_values,
                hidden_states=outputs.hidden_states,
                attentions=outputs.attentions,
            )

@auto_docstring(
    custom_intro="""
    The Qwen3 Model transformer with a sequence classification head on top (linear layer).

    [`Qwen3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.

    Since it does classification on the last token, it requires to know the position of the last token. If a
    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
    each row of the batch).
    """
)
class Qwen3ForSequenceClassification(Qwen3PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = Qwen3Model(config)
        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> SequenceClassifierOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        transformer_outputs: BaseModelOutputWithPast = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )
        hidden_states = transformer_outputs.last_hidden_state
        logits = self.score(hidden_states)

        if input_ids is not None:
            batch_size = input_ids.shape[0]
        else:
            batch_size = inputs_embeds.shape[0]

        if self.config.pad_token_id is None and batch_size != 1:
            raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
        if self.config.pad_token_id is None:
            last_non_pad_token = -1
        elif input_ids is not None:
            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id
            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)
            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)
            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)
        else:
            last_non_pad_token = -1
            logger.warning_once(
                f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
                "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
            )

        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)

        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=pooled_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )


@auto_docstring
class Qwen3ForTokenClassification(Qwen3PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = Qwen3Model(config)
        if getattr(config, "classifier_dropout", None) is not None:
            classifier_dropout = config.classifier_dropout
        elif getattr(config, "hidden_dropout", None) is not None:
            classifier_dropout = config.hidden_dropout
        else:
            classifier_dropout = 0.1
        self.dropout = nn.Dropout(classifier_dropout)
        self.score = nn.Linear(config.hidden_size, config.num_labels)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> TokenClassifierOutput:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        outputs: BaseModelOutputWithPast = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )
        sequence_output = outputs.last_hidden_state
        sequence_output = self.dropout(sequence_output)
        logits = self.score(sequence_output)

        loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.config)

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


@auto_docstring
class Qwen3ForQuestionAnswering(Qwen3PreTrainedModel):
    base_model_prefix = "transformer"

    def __init__(self, config):
        super().__init__(config)
        self.transformer = Qwen3Model(config)
        self.qa_outputs = nn.Linear(config.hidden_size, 2)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.transformer.embed_tokens

    def set_input_embeddings(self, value):
        self.transformer.embed_tokens = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        start_positions: Optional[torch.LongTensor] = None,
        end_positions: Optional[torch.LongTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        **kwargs,
    ) -> QuestionAnsweringModelOutput:
        outputs: BaseModelOutputWithPast = self.transformer(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        sequence_output = outputs.last_hidden_state

        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1).contiguous()
        end_logits = end_logits.squeeze(-1).contiguous()

        loss = None
        if start_positions is not None and end_positions is not None:
            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)

        return QuestionAnsweringModelOutput(
            loss=loss,
            start_logits=start_logits,
            end_logits=end_logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


__all__ = [
    "Qwen3ForCausalLM",
    "Qwen3ForQuestionAnswering",
    "Qwen3Model",
    "Qwen3PreTrainedModel",
    "Qwen3ForSequenceClassification",
    "Qwen3ForTokenClassification",
]

